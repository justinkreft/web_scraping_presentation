{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/title.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from scrapy.selector import Selector\n",
    "import spacy\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "logging.info(\"Preparing imports and log settings for presentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/intro.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/caveat.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok Fantastic! We will be good citizens of the Internets.\n",
    "<img src=\"files/all_the_things.gif\">\n",
    "### Now, gimmie all descriptions of all movies streaming from all the services!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplistic model of a web request\n",
    "<img src=\"files/xkcd-full.png\">\n",
    "<br />\n",
    "* A request is made by a client to a server, \n",
    "* is interpreted by the server, \n",
    "* which prepares -> delivers a complete static response \n",
    "* to the client for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# http://docs.python-requests.org/en/master/\n",
    "response = requests.get('https://www.rottentomatoes.com/browse/dvd-streaming-all/')\n",
    "print(response.status_code)\n",
    "print(response.headers['content-type'])\n",
    "print(response.encoding)\n",
    "print(response.text[:1000] + \" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Content\n",
    "#### But it isn't the 2000's anymore. There is WAY more going on under the hood than just a call and response.\n",
    "\n",
    "#### Examine Rotten Tomatoes in Chrome Inspector\n",
    "https://www.rottentomatoes.com/browse/dvd-streaming-all/\n",
    "\n",
    "Notes:\n",
    "* Generating an exact html payload to client on demand at scale can be strenuous on a server \n",
    "* Modern servers tend to follow highly templatized patterns\n",
    "* The contents of these templates are then populated by various network calls resulting in dynamic content that is\n",
    "    * Either fed in bulk via Javascript on the page\n",
    "    * or requested as necessary through XHR network calls\n",
    "    \n",
    "### So what you are more likely to encounter is\n",
    "<img src=\"files/xkcd-full (copy).png\">\n",
    "#### \"Sure Dude. Here is a template and a bunch of instructions to request additional resources so you can populate the template yourself... because I'm too lazy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If attempting to build a sustainible webscraper we want to\n",
    "* make as few calls as possible\n",
    "* get the most data that we are interested in as possible per call\n",
    "* interact with content whose organization structure is unlikely to change over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing an HTML response\n",
    "The most effective ways to parse content from a single static HTML response programatically are:\n",
    "* xpath queries\n",
    "* regex pattern mining\n",
    "* directly calling ajax/xhr network calls\n",
    "* AVOIDING dynamic scraping like the plague\n",
    "    * Directly calling a network call is the most ideal if available.\n",
    "\n",
    "*Words on why dynamic crawlers are bad*\n",
    "\n",
    "### Interactive demo\n",
    "* Demonstrate xpath usage with xpathHelper tool in Chrome\n",
    "    * `//a/@href`\n",
    "    * `/html/body[@class='body  ']/div[@class='body_main container']/div[@id='main_container']/div[@id='main-row']/div/div[@id='content-column']/div[2]/div[@class='mb-movies']/div[@class='mb-movie'][1]/div[@class='poster_container']/a/@href`\n",
    "    * `//div[@class='poster_container']/a/@href`\n",
    "    * `//div[contains(@class, 'poster_container')]/a/@href`\n",
    "* Look at script objects in the DOM for additional regex options\n",
    "    * `jsonLdSchema\">({.*})<`\n",
    "\n",
    "### Careful! \n",
    "*But what works in client browser is not always what you see in delivered in a simple request that doesn't run the javascript*\n",
    "\n",
    "*Plus we have the problem with that pesky dynamic lazyloaded data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the xpath options\n",
    "# making a quick scrapy selector from response.txt - this is inherited normally by scrapy Response objects\n",
    "selector = Selector(text=response.text, type='html')\n",
    "print(len(selector.xpath(\"//div[contains(@class, 'poster_container')]/a/@href\")), \" -- We expected 32 from xpath helper\")\n",
    "print(len(selector.xpath(\"//a/@href\")), \" -- We expected 228 from xpath helper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what just happend???\n",
    "print(selector.xpath(\"//div[@class='mb-movies list-view']\").extract())\n",
    "selector.xpath(\"//a/@href\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the regex options\n",
    "print(re.findall(r'Papillon', response.text), \" -- We expected 7 from Chrome inspector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.search(r'jsonLdSchema\">({.*})<', response.text).group(1))\n",
    "# Now we are in business\n",
    "json_obj = json.loads(re.search(r'jsonLdSchema\">({.*})<', response.text).group(1))\n",
    "json_obj['itemListElement'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likewise, we could take advantage of that network call\n",
    "# https://www.rottentomatoes.com/api/private/v2.0/browse?maxTomato=100&maxPopcorn=100&services=amazon%3Bhbo_go%3Bitunes%3Bnetflix_iw%3Bvudu%3Bamazon_prime%3Bfandango_now&certified&sortBy=release&type=dvd-streaming-all&page=1\n",
    "# Tip, you can use a nice json viewer like http://jsonviewer.stack.hu/ to explore the object\n",
    "json_response = requests.get('https://www.rottentomatoes.com/api/private/v2.0/browse?maxTomato=100&maxPopcorn=100&services=amazon%3Bhbo_go%3Bitunes%3Bnetflix_iw%3Bvudu%3Bamazon_prime%3Bfandango_now&certified&sortBy=release&type=dvd-streaming-all&page=1')\n",
    "json_data = json.loads(json_response.text)\n",
    "print(json_data.keys())\n",
    "print(json_data['counts'])\n",
    "print(len(json_data['results']))\n",
    "print(json_data['results'][0]['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a spider\n",
    "For our purposes, we can use the network calls to get everything we need, however, frequently spiders use all of the methods described above for different purposes. Also note, this particular spider is pretty straightforward and now that we found a pattern much of what we are going to do might be accomplished with curl requests. But then we wouldn't get a lot of other benefits that scrapy provides below.\n",
    "\n",
    "## Enter Scrapy\n",
    "* a highly extensible asynchronus framework\n",
    "* generally low memory demand\n",
    "* handels all request cue and item processing scheduling\n",
    "* many middleware supports baked in for simplifying \n",
    "   * proxy management\n",
    "   * cacheing pages\n",
    "   * retry logic\n",
    "   * redirect management\n",
    "   * autothrottling requests\n",
    "   * useragent string management\n",
    "* it is maybe 20 times faster than Selenium (even without dynamic crawling)\n",
    "\n",
    "\"If you are building something robust and want to make it as efficient as possible with lots of flexibility and a bunch of functions, and a project use case requires longterm maintence then you should definitely use it.\"\n",
    "\n",
    "### Demo Scrapy in repo\n",
    "*References*\n",
    "* Scrapy - https://doc.scrapy.org/en/latest/\n",
    "* Xpath - https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html\n",
    "* Regex - https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example stats output\n",
    "\"\"\"\n",
    "2018-11-25 11:06:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/exception_count': 4,\n",
    " 'downloader/exception_type_count/twisted.internet.error.NoRouteError': 3,\n",
    " 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,\n",
    " 'downloader/request_bytes': 10470750,\n",
    " 'downloader/request_count': 11635,\n",
    " 'downloader/request_method_count/GET': 11635,\n",
    " 'downloader/response_bytes': 381379582,\n",
    " 'downloader/response_count': 11631,\n",
    " 'downloader/response_status_count/200': 10417,\n",
    " 'downloader/response_status_count/301': 1152,\n",
    " 'downloader/response_status_count/404': 62,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2018, 11, 25, 16, 6, 45, 912498),\n",
    " 'httperror/response_ignored_count': 62,\n",
    " 'httperror/response_ignored_status_count/404': 62,\n",
    " 'item_scraped_count': 9922,\n",
    " 'log_count/CRITICAL': 7,\n",
    " 'log_count/DEBUG': 21558,\n",
    " 'log_count/INFO': 276,\n",
    " 'memusage/max': 1213353984,\n",
    " 'memusage/startup': 992456704,\n",
    " 'request_depth_max': 2,\n",
    " 'response_received_count': 10479,\n",
    " 'retry/count': 4,\n",
    " 'retry/reason_count/twisted.internet.error.NoRouteError': 3,\n",
    " 'retry/reason_count/twisted.internet.error.TimeoutError': 1,\n",
    " 'scheduler/dequeued': 11634,\n",
    " 'scheduler/dequeued/memory': 11634,\n",
    " 'scheduler/enqueued': 11634,\n",
    " 'scheduler/enqueued/memory': 11634,\n",
    " 'start_time': datetime.datetime(2018, 11, 25, 12, 30, 18, 928327)}\n",
    "2018-11-25 11:06:45 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what do I do with this unstructured data? Structure it with NLP\n",
    "Natural Language Processing is the process programming computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "NLP starts with data expressed in natural language. It is unstructured and very difficult for machines to parse. Typical steps to processing unstrucutred data are\n",
    "* cleaning text\n",
    "* stop word removal\n",
    "* parsing sentences\n",
    "* parsing tokens (tokenization)\n",
    "* part of speech tagging (PoS)\n",
    "* lemmatization (word stemming)\n",
    "* n-gram parsing\n",
    "* entitiy recognition\n",
    "* word dependencies\n",
    "* sense disambiguation\n",
    "* sentiment/opion analysis\n",
    "* word embeddings\n",
    "\n",
    "We won't have time, whatsoever to delve into any of these topics. But there is a library worth exploring that provides key entry points to each of these topics: Spacy. At least for most of the parsing steps above. https://spacy.io/\n",
    "\n",
    "A thorough introduction to NLP would walk you through the concepts of\n",
    "* corpus analysis\n",
    "* bag of words representations\n",
    "* Tf-IDF (term-frequency inverse document frequency)\n",
    "* document clustering\n",
    "* similarity measurements\n",
    "* various NLP specific models\n",
    "\n",
    "Using these models, we could create vectors of the text we scrape that could then be fed into\n",
    "* Topic analysis algorithms\n",
    "* Machine learning classifiers\n",
    "* Features in a Neural Network\n",
    "* Recomendation engines\n",
    "* Tuning search engine appications\n",
    "\n",
    "ect. etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just say we wanted to generate some pre-trained vectors of the text_blob we extracted above \n",
    "# to use for a subsequent hackathon event\n",
    "# must have `python -m spacy download en_core_web_md` installed in your environment\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_chronicles = nlp(\"\"\"THE CHRISTMAS CHRONICLES, a holiday adventure from producer Chris Columbus (\"Home Alone\", \"Harry Potter and the Sorcerer's Stone\") and director Clay Kaytis (\"The Angry Birds Movie\"), tells the story of sister and brother, Kate (Darby Camp) and Teddy Pierce (Judah Lewis), whose Christmas Eve plan to catch Santa Claus (Kurt Russell) on camera turns into an unexpected journey that most kids could only dream about. After staking out Santa's arrival, they sneak into his sleigh, cause it to crash and nearly derail Christmas. As their wild night unfolds, Kate and Teddy work together with Santa - as you've never seen him before - and his loyal Elves to save Christmas before it's too late.\n",
    "Rating: NR\n",
    "Genre: Animation, Comedy, Kids & Family\n",
    "Directed By: Clay Kaytis\n",
    "Written By: \n",
    "On Disc/Streaming: Nov 22, 2018\n",
    "Studio: Netflix\"\"\")\n",
    "christmas_chronicles.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[nlp.vocab.strings[x] for x in christmas_chronicles.to_array(['lemma'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[nlp.vocab.strings[x] for x in christmas_chronicles.to_array(['pos'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_chronicles = nlp(\"\"\"THE CHRISTMAS CHRONICLES, a holiday adventure from producer Chris Columbus (\"Home Alone\", \"Harry Potter and the Sorcerer's Stone\") and director Clay Kaytis (\"The Angry Birds Movie\"), tells the story of sister and brother, Kate (Darby Camp) and Teddy Pierce (Judah Lewis), whose Christmas Eve plan to catch Santa Claus (Kurt Russell) on camera turns into an unexpected journey that most kids could only dream about. After staking out Santa's arrival, they sneak into his sleigh, cause it to crash and nearly derail Christmas. As their wild night unfolds, Kate and Teddy work together with Santa - as you've never seen him before - and his loyal Elves to save Christmas before it's too late.\n",
    "Rating: NR\n",
    "Genre: Animation, Comedy, Kids & Family\n",
    "Directed By: Clay Kaytis\n",
    "Written By: \n",
    "On Disc/Streaming: Nov 22, 2018\n",
    "Studio: Netflix\"\"\")\n",
    "\n",
    "outlaw_king = nlp(\"\"\"OUTLAW KING tells the untold, true story of Robert the Bruce who transforms from defeated nobleman to outlaw hero during the oppressive occupation of medieval Scotland by Edward I of England. Despite grave consequences, Robert seizes the Scottish crown and rallies an impassioned group of men to fight back against the mighty army of the tyrannical King and his volatile son, the Prince of Wales. Filmed in Scotland, OUTLAW KING reunites director David Mackenzie (Hell or High Water) with star Chris Pine alongside Aaron Taylor-Johnson, Florence Pugh and Billy Howle.\n",
    "Rating: R (for sequences of brutal war violence some sexuality, language and brief nudity)\n",
    "Genre: Action & Adventure, Drama\n",
    "Directed By: David Mackenzie\n",
    "Written By: Bathsheba Doran, James MacInnes, David Mackenzie, Mark Bomback, David Harrower\n",
    "In Theaters: Nov 9, 2018  Limited\n",
    "On Disc/Streaming: Nov 9, 2018\n",
    "Runtime: 117 minutes\n",
    "Studio: Netflix\"\"\")\n",
    "\n",
    "incredibles = nlp(\"\"\"Everyone's favorite family of superheroes is back in \"Incredibles 2\"--but this time Helen (voice of Holly Hunter) is in the spotlight, leaving Bob (voice of Craig T. Nelson) at home with Violet (voice of Sarah Vowell) and Dash (voice of Huck Milner) to navigate the day-to-day heroics of \"normal\" life. It's a tough transistion for everyone, made tougher by the fact that the family is still unaware of baby Jack-Jack's emerging superpowers. When a new villain hatches a brilliant and dangerous plot, the family and Frozone (voice of Samuel L. Jackson) must find a way to work together again--which is easier said than done, even when they're all Incredible.\n",
    "Rating: PG (for action sequences and some brief mild language)\n",
    "Genre: Action & Adventure, Animation, Kids & Family\n",
    "Directed By: Brad Bird\n",
    "Written By: Brad Bird\n",
    "In Theaters: Jun 15, 2018  Wide\n",
    "On Disc/Streaming: Oct 23, 2018\n",
    "Runtime: 118 minutes\n",
    "Studio: Disney/Pixar\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc some similarities. A higher score is more similar\n",
    "print(christmas_chronicles.similarity(outlaw_king))\n",
    "print(christmas_chronicles.similarity(incredibles))\n",
    "assert christmas_chronicles.similarity(incredibles) > christmas_chronicles.similarity(outlaw_king)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveat: The extremely high sim scores here are related to\n",
    "* use of a pretrained model rather than the corpus we are working with\n",
    "* no use of feature selection or reduction of noise in comparisons\n",
    "* lack of weighted features (i.e. we would do even better if weighting Genre and Rating\n",
    "\n",
    "This was only for demonstration purposes. I would not use this without some significant feature selection.\n",
    "\n",
    "*See the MovieNLPPipeline in this repo for example of application.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/end.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
